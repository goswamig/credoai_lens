{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af84a3a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Customizing Lens\n",
    "\n",
    "Lens strives to give you sensible defaults and automatically do the proper assessments whenever possible. However, there are many times where you'll want to change, select, or extend the functionality. Lens is intended to be an _extensible framework_ that can accomodate your own analysis plan.\n",
    "\n",
    "In this document we will describe a number of ways you can customize Lens:\n",
    "* Setting up CredoModels\n",
    "* Selecting which assessments to run\n",
    "* Parameterizing assessments\n",
    "* Creating new modules and assessments\n",
    "* Incorporating new metrics\n",
    "\n",
    "### Find the code\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/lens_customization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9dfb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import credoai.lens as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58bd8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "We'll set up data and a model to run through some customization options. See `quickstart` for more information on this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d906e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports for example data and model training\n",
    "from credoai.data import fetch_creditdefault\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from copy import deepcopy\n",
    "# data\n",
    "data = fetch_creditdefault()\n",
    "df = data['data']\n",
    "df['target'] = data['target'].astype(int)\n",
    "# model\n",
    "model = GradientBoostingClassifier()\n",
    "X = df.drop(columns=['SEX', 'target'])\n",
    "y = df['target']\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54dbc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_key='SEX',\n",
    "                          label_key='target'\n",
    "                         )\n",
    "alignment_spec = {'Fairness': {'metrics': ['precision_score']}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e3d51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selecting Assessments\n",
    "\n",
    "**Automatic selection of assessments**\n",
    "\n",
    "Lens has a number of assessments available, each of which works with different kinds of models or datasets. By default, Lens will automatically run every assessment that has its prerequesites met. The prerequesities can be queried by calling the `get_requirements` function on an assessment. These indicate the set of features or functions your model and data must instantiate in order for the assessment to be run. \n",
    "\n",
    "These requirements are specific for the model and the data. Each requirement is either a single requirements or a tuple. If a tuple, only one of the requirements within the tuple must be met. For instance, the FairnessAssessment needs *either* `predict_proba` OR `predict`. See `credoai.assessments.credo_assessment.AssessmentRequirements` for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7137eee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_requirements': [('predict_proba', 'predict')],\n",
       " 'data_requirements': ['X', 'y', 'sensitive_features']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment import FairnessAssessment\n",
    "assessment = FairnessAssessment()\n",
    "assessment.get_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad1643",
   "metadata": {},
   "source": [
    "You can also get the requirements for all assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c803c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DatasetFairnessAssessment': {'model_requirements': [],\n",
       "  'data_requirements': ['X', 'y', 'sensitive_features']},\n",
       " 'FairnessAssessment': {'model_requirements': [('predict_proba', 'predict')],\n",
       "  'data_requirements': ['X', 'y', 'sensitive_features']},\n",
       " 'NLPEmbeddingBiasAssessment': {'model_requirements': ['embedding_fun'],\n",
       "  'data_requirements': []},\n",
       " 'NLPGeneratorAssessment': {'model_requirements': ['generator_fun'],\n",
       "  'data_requirements': []},\n",
       " 'PerformanceAssessment': {'model_requirements': [('predict_proba', 'predict')],\n",
       "  'data_requirements': ['X', 'y']}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment import get_assessment_requirements\n",
    "get_assessment_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbecc367",
   "metadata": {},
   "source": [
    "**Selecting a subset of assessments**\n",
    "\n",
    "But what if you don't want Lens to select assessments automatically? No problem! Just pass the assessments you do want to Lens - which will restrict Lens to only use those assessments. Lens will still check to make sure these assessments can be run based on your credo_model and credo_data, and log a warning if it failed to run a particular assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d8b886a-30ad-4213-8071-dace750f5579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DatasetFairness': <credoai.assessment.assessments.DatasetFairnessAssessment at 0x2946e3310>,\n",
       " 'Fairness': <credoai.assessment.assessments.FairnessAssessment at 0x2946e3850>,\n",
       " 'Performance': <credoai.assessment.assessments.PerformanceAssessment at 0x2946e3250>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment import get_usable_assessments, ASSESSMENTS\n",
    "ASSESSMENTS\n",
    "get_usable_assessments(credo_model, credo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74415ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Automatically Selected Assessments for validation dataset: UCI-credit-default\n",
      "--DatasetFairness\n",
      "--Fairness\n",
      "--Performance\n",
      "INFO:absl:Initializing assessments for validation dataset: UCI-credit-default\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "Assessment (Performance) could not be initialized.Ensure the assessment spec is passing the required parameters",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Credo/credoai_lens/credoai/lens.py:346\u001b[0m, in \u001b[0;36mLens._init_assessments\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 346\u001b[0m     \u001b[43massessment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: init_module() missing 1 required keyword-only argument: 'metrics'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Note: Remember to instantiate the assessment!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m lens \u001b[38;5;241m=\u001b[39m \u001b[43mcl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredo_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m               \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredo_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m               \u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malignment_spec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Credo/credoai_lens/credoai/lens.py:128\u001b[0m, in \u001b[0;36mLens.__init__\u001b[0;34m(self, governance, spec, assessments, model, data, training_data, user_id, dev_mode, logging_level, warning_level)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_spec(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec, spec)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# initialize\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_assessments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Credo/credoai_lens/credoai/lens.py:348\u001b[0m, in \u001b[0;36mLens._init_assessments\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m     assessment\u001b[38;5;241m.\u001b[39minit_module(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssessment (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00massessment\u001b[38;5;241m.\u001b[39mget_name()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) could not be initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsure the assessment spec is passing the required parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m                           )\n",
      "\u001b[0;31mValidationError\u001b[0m: Assessment (Performance) could not be initialized.Ensure the assessment spec is passing the required parameters"
     ]
    }
   ],
   "source": [
    "# Note: Remember to instantiate the assessment!\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f342cf5-3191-4b3e-8c14-baf56b6c9050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Remember to instantiate the assessment!\n",
    "selected_assessments = [FairnessAssessment]\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessments=selected_assessments,\n",
    "               spec=alignment_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a9a42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Parameterizing Assessments\n",
    "\n",
    "Now that we can select assessments, how about customizing them? There are two places where assessments can be customized: (1) when their underlying module is initialized and (2) when they are ran (which runs the underlying module).\n",
    "\n",
    "### Customizing initialization\n",
    "Below we can see how to customize the initialization of the assessment. We use something we've already seen before - the `alignment spec`! The parameters that can be passed at this stage are the same parameters passed to the assessment's `init_module` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Remember to instantiate the assessment!\n",
    "selected_assessments = [FairnessAssessment]\n",
    "init_kwargs = deepcopy(alignment_spec)\n",
    "init_kwargs['Fairness']['metrics'].append('recall_score')\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessments=selected_assessments,\n",
    "               spec=init_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab30385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the fairness results, from the Fairness assessment, run on the validation dataset\n",
    "lens.run_assessments().get_results()['validation']['Fairness']['fairness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fdaabd",
   "metadata": {},
   "source": [
    "### Customizing running assessments \n",
    "\n",
    "The other way of parameterizing the assessments is by passing arguments to the assessment's `run` function. These kwargs are passed to `lens.run_assessments`, which are, in turn passed to the assessment's initialized module.\n",
    "\n",
    "For instance, the `Fairness` assessment initializes `mod.Fairness`, whose `run` argument can take a `method` parameter which controls how fairness scores are calculated. The default is \"between_groups\", but we can change it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85708998",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_kwargs = {'Fairness': {'method': 'to_overall'}}\n",
    "lens.run_assessments(assessment_kwargs = run_kwargs).get_results()['validation']['Fairness']['fairness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb7213",
   "metadata": {},
   "source": [
    "## Creating New Modules & Assessments\n",
    "\n",
    "WIP section!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5fefd8",
   "metadata": {},
   "source": [
    "## Module Specific Customization - Custom Metrics for Fairness Base\n",
    "\n",
    "Each module has different parameterization options, as discused above. The FairnessModule takes a set of metrics to calculate on the model and data. Many metrics are supported out-of-the-box. These metrics can be referenced by string. However, custom metrics can be created as well. Doing so will allow you to calculate any metric that takes in a `y_true` and some kind of prediction\n",
    "\n",
    "Custom metrics can be incorporated by creating a `Metric` object. `Metrics` are lightweight wrapper classes that defines a few characteristics of the custom function needed by Lens. \n",
    "\n",
    "**Example: Confidence Intervals**\n",
    "\n",
    "We will create custom metrics that reflect the lower and upper 95th percentile confidence bound on the true positive rate.\n",
    "\n",
    "Confidence intervals are not generically supported. However, they can be derived for metrics derived from a confusion matrix using the `wilson confidence interval`. A convenience function called `confusion_wilson` is supplied which returns an array: [lower, upper] bound for the metric. \n",
    "\n",
    "Wrapping the confusion wilson function in a `Metric` allows us to pass it as a metric to the FairnessModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d83ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.metrics.credoai_metrics import confusion_wilson\n",
    "from credoai.metrics import Metric\n",
    "\n",
    "# define partial functions for the true positive rate lower bound\n",
    "def lower_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='true_positive_rate', confidence=0.95)[0]\n",
    "\n",
    "# and upper bound\n",
    "def upper_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='true_positive_rate', confidence=0.95)[1]\n",
    "\n",
    "# wrap the functions in fairness functions\n",
    "lower_metric = Metric(name = 'lower_bound_tpr', \n",
    "                      metric_category = \"binary_classification\",\n",
    "                      fun = lower_bound_tpr)\n",
    "\n",
    "upper_metric = Metric(name = 'upper_bound_tpr', \n",
    "                      metric_category = \"binary_classification\",\n",
    "                      fun = upper_bound_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a44df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Remember to instantiate the assessment!\n",
    "selected_assessments = [FairnessAssessment()]\n",
    "init_kwargs = deepcopy(alignment_spec)\n",
    "init_kwargs['Fairness']['metrics'] = [lower_metric, 'tpr', upper_metric]\n",
    "custom_lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessments=selected_assessments,\n",
    "               spec=init_kwargs)\n",
    "custom_lens.run_assessments().get_results()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d7eb8a87bb83596f4cd5aeb66d856dad2a9bb65fe804cea051250e36746a46f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
