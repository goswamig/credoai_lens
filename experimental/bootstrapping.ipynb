{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6415197",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting \"Practical\" Significance from Statistical Significance\n",
    "\n",
    "This notebook outlines a technique to say whether two metrics are \"significantly\" different from each other. In other words, if you see a different performance between two groups (like male vs female) is it a \"BAD\" difference? Should you care?\n",
    "\n",
    "To answer this question we bring together statistics and a risk assessment! We need to probe the owners of the AI system for their risk tolerance, and use that to contextualize any differences we see.\n",
    "\n",
    "If you want to jump with a toy you can play with, go [to this cell](#Full-on-Toy-to-play-with)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948442b6-36ed-438a-b956-bc4244b79cd3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa46cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import *\n",
    "from scipy import stats\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooled_sd(arr1, arr2):\n",
    "    n1 = len(arr1)\n",
    "    n2 = len(arr2)\n",
    "    sd1 = np.std(arr1)\n",
    "    sd2 = np.std(arr2)\n",
    "    return (((n1 - 1)*sd1**2 + (n2-1)*sd2**2) / (n1 + n2-2))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33799588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_indices(arr, n=1000, sample_size=None):\n",
    "    if sample_size is None:\n",
    "        sample_size = len(arr)\n",
    "    return np.random.choice(np.arange(len(arr)), size=[sample_size,n], replace=True)\n",
    "\n",
    "def bootstrap_samples(y_pred, y_true=None, n=1000, sample_size=None):\n",
    "    indices = get_bootstrap_indices(y_pred, n=n, sample_size=sample_size)\n",
    "    out = {'y_pred_bootstrap': y_pred[indices]}\n",
    "    if y_true is not None:\n",
    "        out['y_true_bootstrap'] = y_true[indices]\n",
    "    return out\n",
    "\n",
    "def apply_fun(bootstrap_samples, fun, requires_y_true=True):\n",
    "    if requires_y_true:\n",
    "        zipped = zip(bootstrap_samples['y_true_bootstrap'].T, \n",
    "                     bootstrap_samples['y_pred_bootstrap'].T)\n",
    "        return np.array([fun(true, pred) for true, pred in zipped])\n",
    "    else:\n",
    "        return np.array([fun(s) for s in bootstrap_samples['y_pred_bootstrap'].T])\n",
    "\n",
    "def cohens_d(arr1, arr2, verbose=False):\n",
    "    if verbose:\n",
    "        print(f'Mean Statistic 1: {np.mean(arr1)}', \n",
    "              f'Mean Statistic 2: {np.mean(arr2)}')\n",
    "    return (np.mean(arr1)-np.mean(arr2)) / pooled_sd(arr1, arr2)\n",
    "\n",
    "def compare(samples1, samples2, fun, requires_y_true=None, verbose=True):\n",
    "    if requires_y_true is None:\n",
    "        requires_y_true = False\n",
    "        if 'y_true_bootstrap' in samples1:\n",
    "            requires_y_true = True\n",
    "    out1 = apply_fun(samples1, fun, requires_y_true)\n",
    "    out2 = apply_fun(samples2, fun, requires_y_true)\n",
    "    out = cohens_d(out1, out2, verbose=verbose)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c9e35",
   "metadata": {},
   "source": [
    "## Simulation 1\n",
    "\n",
    "Let's just get some basic bootstrapping working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = lambda size: np.random.normal(loc=2, \n",
    "                                      scale=1, \n",
    "                                      size=size)\n",
    "dist2 = lambda size: np.random.normal(loc=0, \n",
    "                                      scale=1, \n",
    "                                      size=size)\n",
    "val1 = dist1(10000)\n",
    "val2 = dist2(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c5978",
   "metadata": {},
   "source": [
    "### simple case - calculate whether distributions are different\n",
    "\n",
    "* Sample each distribution N times\n",
    "* calculate the effect size between the two distributions\n",
    "* this gives you CI around the effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples1 = bootstrap_samples(val1, sample_size=None)\n",
    "samples2 = bootstrap_samples(val2, sample_size=None)\n",
    "\n",
    "tmp1 = samples1['y_pred_bootstrap']\n",
    "tmp2 = samples2['y_pred_bootstrap']\n",
    "\n",
    "effect_sizes = [cohens_d(tmp1[:,i], tmp2[:, i]) for i in range(tmp1.shape[1])]\n",
    "h=plt.hist(effect_sizes, bins=100)\n",
    "plt.title(np.mean(effect_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588575b3-7b43-45ad-9d4c-ad667ee38425",
   "metadata": {},
   "source": [
    "Boom, it works. We find that the effect size is about 2, as it should be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bcfa98",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What if each sample just gave one number? Like accuracy\n",
    "\n",
    "In this case, you calculate the statistic for each sample. For instance, let's say we get correct or incorrect values for each person and want to see if the overall accuracy differs between groups. We can use the bootstrap to get a distribution of accuracies for each sample. We can compare the two distributions of accuracy, but the distributions move towards a point estimate as the sample size goes up! If we use the original sample sizes we will always get a humongous effect size! This is kind of how it should be however, because ...stats.\n",
    "\n",
    "A proper bootstrap will use the same N as the original example. Otherwise you will underestimate the effect size. But maybe we can use that to our advantage...\n",
    "\n",
    "First, let's go through the normal stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e11a3d-f3ec-4d74-b442-9733e4772bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two binomial distributions (reflecting correct/incorrect for two groups)\n",
    "# one distribution is 80% correct, the other is 70% correct\n",
    "dist1 = lambda size: np.random.binomial(1, .8, size=size)\n",
    "dist2 = lambda size: np.random.binomial(1, .7, size=size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7cc63b-69ce-45dc-a6a3-52a0aa073c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameteric, classic stats with a sample of 100\n",
    "def to_observations(binomial_results):\n",
    "    N = sum(binomial_results)\n",
    "    return [N, len(binomial_results)-N]\n",
    "\n",
    "def calc_statistic(p1, p2, n1, n2):\n",
    "    p_combined = (p1*n1+p2*n2)/(n1+n2)\n",
    "    z = (p1-p2)/(p_combined*(1-p_combined)*(1/n1+1/n2))**.5\n",
    "    p_val = stats.norm.sf(abs(z))*2\n",
    "    return z, p_val\n",
    "\n",
    "repeats = []\n",
    "for _ in range(100):\n",
    "    val1 = dist1(100)\n",
    "    val2 = dist2(100)\n",
    "    # chisquared test\n",
    "    results = {}\n",
    "    out = stats.chi2_contingency([to_observations(val1), to_observations(val2)])\n",
    "    results['chi_sq'] = out[1]\n",
    "\n",
    "    # another test\n",
    "    # see here https://stats.stackexchange.com/questions/113602/test-if-two-binomial-distributions-are-statistically-different-from-each-other\n",
    "    p1 = np.mean(val1)\n",
    "    p2 = np.mean(val2)\n",
    "    n1 = len(val1)\n",
    "    n2 = len(val2)\n",
    "    z, p_val = calc_statistic(p1, p2, n1, n2)\n",
    "    results['z_test'] = p_val\n",
    "    repeats.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe53d77-e5af-4b87-b710-686d845954ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get a range of p vals for each sample\n",
    "statistics = [r['z_test'] for r in repeats]\n",
    "_=plt.hist(statistics, bins=100)\n",
    "plt.title(f'Mean pval: {np.mean(statistics):.4f}')\n",
    "plt.xlabel('pvals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f4abf-6bda-47bb-abdf-7a089221649c",
   "metadata": {},
   "source": [
    "We can also estimate these p-values using the bootstrap. Or get the effect size. This is nice! We don't always have a nice closed-form solution for the difference between two statistics like we did for two binomial distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val1 = dist1(10000)\n",
    "val2 = dist2(10000)\n",
    "n=5000\n",
    "sample_sizes = [10, 100, 200, 1000, None]\n",
    "h= f, axes = plt.subplots(1,len(sample_sizes), figsize=(len(sample_sizes)*4,4))\n",
    "\n",
    "for ax, sample_size in zip(axes, sample_sizes):\n",
    "    sample1 = bootstrap_samples(val1, n=n, sample_size=sample_size)\n",
    "    sample2 = bootstrap_samples(val2, n=n, sample_size=sample_size)\n",
    "    N = len(sample1['y_pred_bootstrap']) # sample size\n",
    "    statistic1 = np.mean(sample1['y_pred_bootstrap'], axis=0)\n",
    "    statistic2 = np.mean(sample2['y_pred_bootstrap'], axis=0)\n",
    "\n",
    "    effect_size = cohens_d(statistic1, statistic2)\n",
    "    p_val = stats.norm.sf(abs(effect_size))*2\n",
    "    ax.hist(statistic1)\n",
    "    ax.hist(statistic2)\n",
    "    ax.set_title(f'Sample Size: {N}\\nEffect Size: {effect_size:.2f}\\np-val: {p_val:f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d2d9d-c6f8-451b-9857-a06e54f29c77",
   "metadata": {},
   "source": [
    "### Probing your subjecive risk\n",
    "\n",
    "Ok, so now we understand that the statistical significance of a comparison between two distributions is a function of sample size. Sample size is the important variable that makes a difference of <-----> this big and turns it into a standardized \"that's pretty big\" or \"that's not very big\". (Note - though I am talking about binaries, you can also think about this all as putting it on a standardized scale of \"bigness\", that is dependent on the sample size).\n",
    "\n",
    "Said another way, once you fix the sample size, we can know whether a difference between two groups is big. Sample size essentially tells me how risky you want to be. If you would care about small differences, you need a large sample size. If you only care about big differences, you need a smaller sample size (this is the idea of [statistical power](https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python/))\n",
    "\n",
    "We're gonna run with this and find a way to convert your subjective risk assessment into a sample size we can use.\n",
    "\n",
    "-------\n",
    "\n",
    "Let's say you want think an \"accuracy difference of 10% is a big deal\"... Well, actually that's not gonna be good enough as a difference in accuracy between 40% and 50% is less impressive than 85% and 95%. \n",
    "\n",
    "So let's be more precise with our example. You think: \"if one group has an accuracy of 80%, the other group can't be more than 10% different, otherwise it's a big deal\". That's a reflection of your subjective risk tolerance. And it says something about what sample size we'd need, so that we could detect the 10% difference (it's about 200 people). We can imagine probing this a number of times, in different ways. For now, let's just assume we have this one question, and from it we will get the person's risk tolerance.\n",
    "\n",
    "What we are trying to do is make _statistical_ significance EQUAL _practical_ signifiance. We will use a \"statistically significant\" result to give us a YES/NO answer on whether the effect we see is a \"big deal\". Statistical significance is a function of **sample size**. By asking the probe above we are told what difference the person considers practically significant, we are able to determine the minimal sample size needed such that their \"practically\" signficant difference is also \"statistically significant\".\n",
    "\n",
    "Let's turn all that into code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883aee9-b474-4d5d-8067-d4ab18fe9fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_statistic(p1, p2, n1, n2):\n",
    "    p_combined = (p1*n1+p2*n2)/(n1+n2)\n",
    "    z = (p1-p2)/(p_combined*(1-p_combined)*(1/n1+1/n2))**.5\n",
    "    p_val = stats.norm.sf(abs(z))*2\n",
    "    return z, p_val\n",
    "\n",
    "# Let's assume both have the same sample size for our two groups for the purpose of this translation. That means we can combine n1 and n2\n",
    "def calc_statistic(p1, p2, n):\n",
    "    p_combined = (p1+p2)/2\n",
    "    z = (p1-p2)/(p_combined*(1-p_combined)*(2/n))**.5\n",
    "    p_val = stats.norm.sf(abs(z))*2\n",
    "    return z, p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f39bcd-1391-43ae-baea-e3255798b666",
   "metadata": {},
   "source": [
    "we can solve for **n**, which allows us to get out a sample size, given a z value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a1f8a-a57d-4c12-a661-3045ab36bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n(p1, p2, test_statistic):\n",
    "    p_combined = (p1+p2)/2\n",
    "    num = p_combined*(1-p_combined)*2\n",
    "    denom = ((p1-p2)/test_statistic)**2\n",
    "    return num/denom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea56620-68dc-4c85-930c-388913cb7270",
   "metadata": {},
   "source": [
    "We are going to define \"a big deal\" as a statistical significance of p<.01. Arbitrary, but makes sense. We could imagine defining a \"medium deal\" as p<.05 or something. Easy to change later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cac287-7656-44e2-9fab-92000e8cb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the analogy\n",
    "base=.8\n",
    "significant_accuracy_diff = .1\n",
    "\n",
    "# define what \"very important\". Let's say \"very important\" means a p-val of .01\n",
    "important_p_val = .01\n",
    "sig_val = stats.norm.ppf(1-important_p_val/2) # zval equal to pval of .05\n",
    "\n",
    "# it's easier to find a signfiicant effect if the deviation is AWAY from 50% than towards it\n",
    "# so we will average up and down together\n",
    "n_up = find_n(base+significant_accuracy_diff, base, sig_val)\n",
    "n_down = find_n(base-significant_accuracy_diff, base, sig_val)\n",
    "average_n = (n_up+n_down)/2\n",
    "print(f'Number of people to use for this test: [{n_down}, {n_up}]\\nAverage: {average_n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07a95c-a9cc-4927-8037-23ca6bfb82d2",
   "metadata": {},
   "source": [
    "Now we have an N. We can use this in the bootstrap for random other metrics and give \"practically significant\" results. \"Practical significance\" is defined as \"statistical significance\" with the number of people we found above.\n",
    "\n",
    "We can imagine giving a short questionnaire (like a investment company does - see vanguard). The questionnaire would give a few examples like above and we would get samples of their \"risk tolerance\". We then apply that to any metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d830a5d6-ef2b-42f6-92b5-64ee5a237b5e",
   "metadata": {},
   "source": [
    "Let's make this a questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86373d59-0f1c-4871-88dc-876989aacadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_questionnaire():\n",
    "    base = .8\n",
    "    important_p_val = .01\n",
    "    sig_val = stats.norm.ppf(1-important_p_val/2) # zval equal to pval of .05\n",
    "\n",
    "    print(\"The priveleged group has an accuracy of 80%\")\n",
    "    significant_accuracy_diff = float(input(\"How much worse would the accuracy for the unpriveleged group have to be to be a 'big deal'\"))/100\n",
    "\n",
    "    n_up = find_n(base+significant_accuracy_diff, base, sig_val)\n",
    "    n_down = find_n(base-significant_accuracy_diff, base, sig_val)\n",
    "    average_n = (n_up+n_down)/2\n",
    "    print(f'Number of people to use for this test: [{n_down}, {n_up}]\\nAverage: {average_n}')\n",
    "    return int(average_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d47f24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simulation - Regression\n",
    "\n",
    "So now let's say we have a real use case. We have two distributions and our \"predictive model\" is worse on one than another. The way it's worse is its predictions are \"noisier\" (we've added a certain amount of gaussian noise). You can imagine one distribution to be the performance of the model on a priveleged group (e.g., white people) and another on an unpriveleged group (e.g. black people). Is the difference between the two models \"bad\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ddeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = int(average_n) # sample size we learned from our risk probe above\n",
    "#config\n",
    "dist1 = lambda size: np.random.exponential(size=size)\n",
    "n1 = 10000\n",
    "n2 = 10000\n",
    "true1, true2 = dist1(n1), dist1(n2)\n",
    "noise1 = .2\n",
    "noise2 = .6\n",
    "\n",
    "pred1 = true1 + np.random.normal(size=n1)*noise1\n",
    "pred2 = true2 + np.random.normal(size=n2)*noise2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023fe781-c7f1-4487-8a92-9dbafb5c87ba",
   "metadata": {},
   "source": [
    "Here's a plot showing that difference. You can see that the cloud for the worse model is broader - it isn't doing as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1,2, figsize=(8,4))\n",
    "axes[0].scatter(true1, pred1, alpha=.3, lw=1, edgecolor='w')\n",
    "axes[0].set_title('better model')\n",
    "axes[1].scatter(true2, pred2, alpha=.3, lw=1, edgecolor='w')\n",
    "axes[1].set_title('worse model')\n",
    "axes[0].set_ylabel('Prediction')\n",
    "axes[0].set_xlabel('Ground Truth')\n",
    "axes[1].set_xlabel('Ground Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768eeff-dc6e-46cf-a275-14114292848b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example\n",
    "\n",
    "In the below example we'll run our questionnaire which will give us the critical \"sample size\" parameter as a function of your risk tolerance.\n",
    "\n",
    "We'll then look at a sequence of model comparisons. The first will have a \"base\" level of noise, and the second will have iteratively more noise added (making it a worse model). At some point, the amount of noise will be \"BAD\" (too much given your risk tolerance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b959e-e836-4aa9-9283-041339dfee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate \n",
    "def evaluate(pred1, true1, pred2, true2, eval_fun, sample_size, n_boot=2000, ax=None):\n",
    "    samples1 = bootstrap_samples(pred1, true1, \n",
    "                                 n_boot, sample_size=sample_size)\n",
    "    samples2 = bootstrap_samples(pred2, true2, \n",
    "                                 n_boot, sample_size=sample_size)\n",
    "    # on each sample compute r2_score\n",
    "    requires_y_true = True\n",
    "    statistic1 = apply_fun(samples1, eval_fun, requires_y_true)\n",
    "    statistic2 = apply_fun(samples2, eval_fun, requires_y_true)\n",
    "\n",
    "    # compare distributions of r2 scores\n",
    "    effect_size = cohens_d(statistic1, statistic2)\n",
    "    p_val = stats.norm.sf(abs(effect_size))*2\n",
    "    answer = 'YES' if p_val > .01 else 'NO'\n",
    "    if ax is not None:\n",
    "        ax.hist(statistic1, bins=50, alpha=.5)\n",
    "        ax.hist(statistic2, bins=50, alpha=.5)\n",
    "        ax.set_title(f'Good?: {answer}\\nSample Size: {sample_size}\\nEffect Size: {effect_size:.2f}\\np-val: {p_val:f}')\n",
    "    return answer, p_val, effect_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c2eed-dcb7-44b8-99a6-f470edf11351",
   "metadata": {},
   "source": [
    "#### Full on Toy to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf6899-5875-4c05-8e65-b274f2bb7f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = run_questionnaire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88594c-7bb8-4b99-930f-096deadf9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_noise = .5\n",
    "noises = np.array([.05, .1, .15, .2, .25])+base_noise\n",
    "\n",
    "pred1 = true1 + np.random.normal(size=n1)*base_noise\n",
    "n = len(noises)\n",
    "f, axes = plt.subplots(1, n, figsize=(4*n,4))\n",
    "for ax, noise in zip(axes, noises):\n",
    "    pred2 = true2 + np.random.normal(size=n2)*noise\n",
    "    evaluate(pred1, true1, pred2, true2, r2_score, sample_size, ax=ax)\n",
    "    ax.set_xlabel(f'Noise: {noise}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb1b92-9fe8-4632-a029-1d1cddf26465",
   "metadata": {},
   "source": [
    "We can plot this out across noise levels. Below is a grid of \"base\" noises (the quality of the predictions on the priveleged group) vs the comparison noise (the quality of predictions for the unpriveleged group).\n",
    "\n",
    "The green boxes are \"GOOD\", the blue ones are \"BAD\". You can see that as the noise of the comparison predictions gets worse the predictions becomes labeled \"BAD\". How much noise is necessary to get a \"BAD\" rating is dependent on how much noise is in the base performance.\n",
    "\n",
    "Intuitively, this makes sense. If the model is GREAT at prediction one group of people (the priveleged group), then a smaller performance decrement is a large _relative_ change. If the model is just ok at prediction with the priveleged group then a larger amount of noise would need to be added to have the same kind of relative change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc250b1-7271-47c1-83c8-a2f2f43065f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_list = np.arange(.05,.8, .025)\n",
    "comparison_list = np.arange(.05,.8, .025)\n",
    "sample_size = run_questionnaire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0a541-2989-4c08-8830-cc6416d3c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10000\n",
    "true1, true2 = dist1(n), dist1(n)\n",
    "\n",
    "results = []\n",
    "for base in base_list:    \n",
    "    pred1 = true1 + np.random.normal(size=n)*base\n",
    "    base_results = [.5]*len(comparison_list)\n",
    "    for i, comparison in enumerate(comparison_list):\n",
    "        if comparison == base:\n",
    "            base_results[i] = 1\n",
    "        if comparison > base:\n",
    "            pred2 = true2 + np.random.normal(size=n)*comparison\n",
    "            a, p, e= evaluate(pred1, true1, pred2, true2, r2_score, sample_size, n_boot=500)\n",
    "            base_results[i] = 1 if a == \"YES\" else 0\n",
    "    results.append(base_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f2e3c9-d334-4794-9900-766a780e3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = pd.DataFrame(results, \n",
    "                  index=[f'{i:.2f}' for i in base_list], \n",
    "                  columns=[f'{i:.2f}' for i in comparison_list])\n",
    "palette = list(sns.color_palette(n_colors=3))\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(df, \n",
    "            cmap=palette, \n",
    "            cbar=False, \n",
    "            mask = df==.5,\n",
    "            square=True, linewidth=1)\n",
    "plt.ylabel('Priveleged Noise')\n",
    "plt.xlabel('Unpriveleged Noise')\n",
    "plt.title('Is the performance difference Good (green)?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab273b5-a82c-4aa6-85f7-a2cf74b48176",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test out script version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a9b97-0910-41fa-a0a7-da4a8a88324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootstrap import Sampler\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from credoai.modules import FairnessModule, FairnessFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e1eff-c9ae-4888-a9b4-c115f469932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire = RiskQuestionnare()\n",
    "sample_sizes = questionnaire.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce5856-6279-411d-ac5f-62fc8e9cfa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred setup\n",
    "pred1 = true1 + np.random.normal(size=n1)*.2\n",
    "pred2 = true2 + np.random.normal(size=n2)*.22\n",
    "\n",
    "#metric setup\n",
    "from functools import partial\n",
    "rmse = partial(mean_squared_error, squared=False)\n",
    "metrics = {'r2_score': r2_score, \n",
    "           'mse': mean_squared_error, \n",
    "           'rmse': rmse,\n",
    "           'mae': mean_absolute_error}\n",
    "wrapped_metrics = {FairnessFunction(k, v) for k, v in metrics.items()}\n",
    "\n",
    "# set up sampler\n",
    "sampler = Sampler()\n",
    "cat_y_true = np.hstack([true1, true2])\n",
    "cat_y_pred = np.hstack([pred1, pred2])\n",
    "sensitive_feature = np.array(['male']*len(true1)+['female']*len(true2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2123a4f9-524a-4f58-8bff-5ee054dde789",
   "metadata": {},
   "source": [
    "### working without fairlearn is much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935a6d5-76ca-442d-bb3f-09d26909983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for _ in range(reps):\n",
    "    a=r2_score(y_true[sensitive=='female'], y_pred[sensitive=='female'])\n",
    "    b=r2_score(y_true[sensitive=='male'], y_pred[sensitive=='male'])\n",
    "    a-b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3bc62-02f9-482c-8aa7-039c25f28ec3",
   "metadata": {},
   "source": [
    "#### Sampling on metric frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53442d9b-27c0-4213-b63c-dd4a5120be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total_out = {}\n",
    "for deal, sample_size in sample_sizes.items():\n",
    "    thresh = questionnaire.deals[deal]\n",
    "\n",
    "    # create samples\n",
    "    n_rep = 1000\n",
    "    samples = sampler.create_samples({'y_true': cat_y_true,\n",
    "                                      'y_pred': cat_y_pred,\n",
    "                                      'sensitive_feature': sensitive_feature},\n",
    "                                     n_rep=n_rep,\n",
    "                                     sample_size=sample_size)\n",
    "    # run fairness\n",
    "    results = []\n",
    "    for rep in range(n_rep):\n",
    "        y_true, y_pred, sensitive = [samples[key][:, rep] \n",
    "                                     for key in ['y_true', 'y_pred', 'sensitive_feature']]\n",
    "        frame = MetricFrame(\n",
    "            metrics = metrics,\n",
    "            y_true = y_true,\n",
    "            y_pred = y_pred,\n",
    "            sensitive_features = sensitive\n",
    "        )\n",
    "        results.append(frame.by_group)\n",
    "    results = pd.concat(results, axis=1)\n",
    "\n",
    "    # eval\n",
    "    out = {}\n",
    "    for metric in metrics.keys():\n",
    "        out[metric] = sampler.evaluate(results.loc['female', metric], \n",
    "                                       results.loc['male', metric], \n",
    "                                       thresh=thresh)\n",
    "    total_out[deal] = out\n",
    "\n",
    "# convert to df\n",
    "all_df = []\n",
    "for deal in total_out.keys():\n",
    "    df = pd.DataFrame(total_out[deal])\n",
    "    all_df.append(pd.concat({deal: df}, names=['Deal']))\n",
    "df = pd.concat(all_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454cd16-90a4-420a-90c4-9a68f1b64607",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = lambda _: 3\n",
    "self.module = FairnessModule\n",
    "\n",
    "def run_bootstrapped(y_true, y_pred, sensitive_features,\n",
    "                     method='between_groups',\n",
    "                     n_rep=1000, sample_size=500, thresh=.01):\n",
    "    arrays = {'y_true': y_true,\n",
    "              'y_pred': y_pred,\n",
    "              'sensitive_feature': sensitive_features}\n",
    "    sampler = Sampler(verbose=False)\n",
    "    samples = sampler.create_samples(arrays, n_rep=n_rep, sample_size=sample_size)\n",
    "    \n",
    "    results = []\n",
    "    for rep in range(n_rep):\n",
    "        y_true, y_pred, sensitive = [samples[key][:, rep] \n",
    "                                     for key in ['y_true', 'y_pred', 'sensitive_feature']]\n",
    "        mod=self.module(wrapped_metrics, sensitive, y_true, y_pred)\n",
    "        results.append(mod.run().get_results())\n",
    "        \n",
    "    # concat results\n",
    "    all_results = {}\n",
    "    for key in results[0].keys():\n",
    "        all_results[key] = pd.concat([r[key] for i, r in enumerate(results)])\n",
    "        \n",
    "    # eval\n",
    "    out = {}\n",
    "    df = all_results['disaggregated_results']\n",
    "    avg_results = df.groupby(df.index).mean()\n",
    "\n",
    "    method = 'between_groups'\n",
    "    for metric in metrics.keys():\n",
    "        if method == 'to_overall':\n",
    "            group1 = 'overall'\n",
    "            group2 = abs(avg_results[metric] - avg_results.loc['overall', metric]).idxmax()\n",
    "        elif method == 'between_groups':\n",
    "            group1 = avg_results[metric].idxmin()\n",
    "            group2 = avg_results[metric].idxmax()\n",
    "        out[metric] = sampler.evaluate(df.loc[group1, metric], \n",
    "                                       df.loc[group2, metric], \n",
    "                                       thresh=thresh)\n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2526cef-e83e-4087-9760-0a9e49664f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "out=run_bootstrapped(cat_y_true, cat_y_pred, sensitive_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6089c7-63d3-47f0-9d17-f5fac8b11cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d34cc-9ab3-4b14-ab68-9c3f3d6894a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'race': ['white', 'white', 'white', 'white', 'white', 'white', 'black', 'asian', 'asian', 'asian', 'asian', 'asian'],\n",
    "                    'gender': ['f', 'f', 'f', 'm', 'f', 'f', 'f', 'f', 'm', 'm', 'm', 'm'],\n",
    "                  'experience': [0, 0.1, 0.2, 0.4, 0.5, 0.6, 0, 0.1, 0.2, 0.4, 0.5, 0.6],\n",
    "                  'mark': [0, 0.3, 0.2, 0.4, 0.5, 0.6, 0.3, 0.1, 0.2, 0.4, 0.9, 0.6],\n",
    "                  'zip code': [22203, 22205, 22203, 22203, 22203, 22203, 22204, 22204, 22204, 22204, 22203, 22204]})\n",
    "# make longer, for sake of demo\n",
    "df = pd.concat([df]*3, axis=0)\n",
    "\n",
    "# convert cat columns to numeric to use\n",
    "cat_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in cat_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    df[f\"{col}_codes\"] = df[col].cat.codes\n",
    "\n",
    "# drop categorical variables, just keep codes\n",
    "processed_df = df.drop(cat_columns, axis=1)\n",
    "\n",
    "# label which ones are categorical\n",
    "cat_columns = ['zip code', 'race_codes', 'gender_codes']\n",
    "discrete_features = [True if col in cat_columns else False \n",
    "                     for col in processed_df.columns]\n",
    "\n",
    "mi= mutual_info_classif(processed_df, df['gender_codes'], \n",
    "                        discrete_features=discrete_features,random_state=42)\n",
    "pd.Series(mi, index=processed_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090cf6f9-23a8-45fc-b5dc-8bfd58128015",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84161774-ca06-41bb-ae1e-0587ac5d9324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9f7bc4b-d650-4f4d-a20e-9e46a2867858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "experience      0.226654\n",
       "mark            0.356054\n",
       "zip code        0.118545\n",
       "race_codes      0.361328\n",
       "gender_codes    1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_selection import *\n",
    "\n",
    "df = pd.DataFrame({'race': ['white', 'white', 'white', 'white', 'white', 'white', 'black', 'asian', 'asian', 'asian', 'asian', 'asian'],\n",
    "                    'gender': ['f', 'f', 'f', 'm', 'f', 'f', 'f', 'f', 'm', 'm', 'm', 'm'],\n",
    "                  'experience': [0, 0.1, 0.2, 0.4, 0.5, 0.6, 0, 0.1, 0.2, 0.4, 0.5, 0.6],\n",
    "                  'mark': [0, 0.3, 0.2, 0.4, 0.5, 0.6, 0.3, 0.1, 0.2, 0.4, 0.9, 0.6],\n",
    "                  'zip code': [22203, 22205, 22203, 22203, 22203, 22203, 22204, 22204, 22204, 22204, 22203, 22204]})\n",
    "# make longer, for sake of demo\n",
    "df = pd.concat([df]*3, axis=0)\n",
    "\n",
    "# convert cat columns to numeric to use\n",
    "cat_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in cat_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    df[f\"{col}_codes\"] = df[col].cat.codes\n",
    "\n",
    "# drop categorical variables, just keep codes\n",
    "processed_df = df.drop(cat_columns, axis=1)\n",
    "\n",
    "# label which ones are categorical\n",
    "cat_columns = ['zip code', 'race_codes', 'gender_codes']\n",
    "discrete_features = [True if col in cat_columns else False \n",
    "                     for col in processed_df.columns]\n",
    "\n",
    "mi = mutual_info_classif(processed_df, df['gender_codes'], \n",
    "                        discrete_features=discrete_features,random_state=42)\n",
    "out = pd.Series(mi, index=processed_df.columns)\n",
    "normalized = out/out.max()\n",
    "normalized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
